{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортнем все сразу, чтобы не вспоминать\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import VOCDetection\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection\n",
    "\n",
    "У нас достаточно инструментов и знаний, чтобы попробовать сделать детекцию объектов.\n",
    "\n",
    "## Определимся сначала с постановкой задачи\n",
    "\n",
    "Пусть у нас есть картинки:\n",
    "![Картинки](./img/plain-img.png)\n",
    "\n",
    "Поручим людям пройтись по картинкам и отметить все интересующие нас объекты. Это можно сделать например следующими способами:\n",
    "\n",
    "- boundary box (bbox или bb) -- прямоугольник `[x, y, w, h]` со сторонами параллельными краям картинки (самый простой вариант)\n",
    "- instance/semantic mask -- картинка с нулями и единицами размером с исходную картинку, показывающая какие пиксели относятся к объекту (самый затратный вариант)\n",
    "\n",
    "![Разметка](./img/img-with-annotation.png)\n",
    "\n",
    "Можно размечать иначе под конкретные нужды:\n",
    "- center + radius `[x, y, r]`\n",
    "- rotated bbox `[angle, x, h, w, h]`\n",
    "- ломанная окружающая линия `[(x_0, y_0), ... (x_n, y_n)]`\n",
    "- whatever\n",
    "\n",
    "\n",
    "**Итак: мы хотим сделать сеть, которая будет с одной картинки предсказывать положение (bbox) и класс нескольких объектов (из N классов)**\n",
    "\n",
    "\n",
    "## Интерпретация feature maps\n",
    "\n",
    "![](./img/backbone.png)\n",
    "\n",
    "Если мы прогоним картинку `[3, W, H]` через привычную сверточную сеть (возьмем например классификационную сетку до GAP), мы получим тензор с большим количеством каналов небольшого пространственного размера `[ch, w, h]` (пространственные размеры пускай уменьшились K раз, K=32 для resnet50 **TODO: проверить размеры**).\n",
    "Можно грубо сказать что каждый пиксель выходного тензора отвечает за область `KxK` пикселей входной картинки (рецептивно поле однако накрывает всю картинку с запасом). \n",
    "\n",
    "![](./img/img-with-grid.png)\n",
    "\n",
    "Мы уже пытались интерпретировать пиксели в этом тензоре как Class Activation Map. Показывает что-то интересное, но как детектор явно не получится использовать.\n",
    "Однако мы можем сформулировать детекцию как оптимизационную задачу и проучить модель специально под нее.\n",
    "Давайте пойдем от идеи CAM и повесим на каждый пиксель выходной мапы несколько голов, которые будут предсказывать необходимые для детекции вещи. \n",
    "\n",
    "Договоримся, что пиксель относится к объекту только если его центр (точнее центр BBox'а) попадает в область действия пикселя.\n",
    "\n",
    "![](./img/image-ssd.png)\n",
    "\n",
    "Чтобы завести детекцию нам потребуются такие головы:\n",
    "\n",
    "- `bbox regression` - регрессия bbox'а\n",
    "- `objectness` - относится ли пиксель к объекту или нет\n",
    "- `clf` - если относится, то давайте предскажем класс\n",
    "\n",
    "\n",
    "Как мы когда-то обсуждали, Conv1x1 действует на каждый пиксель точно так же как FC-слой. Так что для наших нужд нам достаточно добавить к модели Conv1x1 с количеством выходных каналов `4 + 1 + N_classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monkey_forward(net, x):\n",
    "    x = net.conv1(x)\n",
    "    x = net.bn1(x)\n",
    "    x = net.relu(x)\n",
    "    x = net.maxpool(x)\n",
    "\n",
    "    x = net.layer1(x)\n",
    "    x = net.layer2(x)\n",
    "    x = net.layer3(x)\n",
    "    x = net.layer4(x)\n",
    "    \n",
    "    x = net.final(x)\n",
    "    # x = net.avgpool(x)\n",
    "    # x = torch.flatten(x, 1)\n",
    "    # x = net.fc(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt torchvision classification model for detection\n",
    "\n",
    "На первый взгляд нам надо только выбросить avgpool, однако [forward метод в torchvision.model.resnet написан неудачно для наших целей](https://pytorch.org/vision/stable/_modules/torchvision/models/resnet.html#resnet18), так что придется его запатчить.\n",
    "\n",
    "**JFYI: подмена логики в рантайме (ака [monkey patching](https://en.wikipedia.org/wiki/Monkey_patch)) -- распространенная но _опасная_ практика**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction transformations\n",
    "\n",
    "**Как превратить предсказанные числа в координаты bbox'ов?**\n",
    "\n",
    "Пусть один пиксель выходного тензора относится к патчу (kx, ky) входной картинки.\n",
    "BBox'ы живут в пространстве исходных картинок, непосредственные выходы сети -- в пространстве выходных тензоров.\n",
    "\n",
    "Для удобства вычислений, давайте представим bbox как координаты центра (так будет удобнее регрессировать) + ширина и высота. Предсказываем 4 числа: $t_x, t_y, t_w, t_h$. `scale` -- это цена выходного пикселя во входных.\n",
    "\n",
    "$$\n",
    "x_c = s \\cdot (\\tanh t_x + i + 0.5)\\\\\n",
    "y_c = s \\cdot (\\tanh t_y + j + 0.5)\\\\\n",
    "w = s \\cdot \\exp{t_w}\\\\\n",
    "h = s \\cdot \\exp{t_h}\n",
    "$$\n",
    "\n",
    "**NB: Обычно используют так называемые anchor -- затравочные bbox'ы, см whiteboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cx torch.Size([1, 1, 13, 8])\n",
      "cy torch.Size([1, 1, 13, 8])\n",
      "ww torch.Size([1, 1, 13, 8])\n",
      "hh torch.Size([1, 1, 13, 8])\n",
      "obj torch.Size([1, 1, 13, 8])\n",
      "cls torch.Size([1, 12, 13, 8])\n"
     ]
    }
   ],
   "source": [
    "class VeryModel(nn.Module):\n",
    "    def __init__(self, n_classes=12, cfg=None):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.cfg = cfg\n",
    "        model = resnet18(pretrained=True)\n",
    "        # добавим в модельку слой и подменим forward\n",
    "#         model.final = nn.Conv2d(model.fc.in_features, 4 + 1 + n_classes, 1)\n",
    "#         model.forward = lambda x: monkey_forward(model, x)\n",
    "        self.inner = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.inner(x)\n",
    "        \n",
    "        return dict(\n",
    "            bboxes=...,\n",
    "            conf=....,\n",
    "            clf=...,\n",
    "        )\n",
    "    \n",
    "    def compute_all(self, batch, device=None):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "net = VeryModel()\n",
    "net = net.eval()\n",
    "with torch.no_grad():\n",
    "    x = torch.zeros((1, 3, 416, 246))\n",
    "    out = net(x)\n",
    "    for k, v in out.items():\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.repeat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./voc/VOCtrainval_11-May-2012.tar\n"
     ]
    }
   ],
   "source": [
    "CLASSES = [\n",
    "    'person', 'chair', 'car', 'dog', 'bottle', 'cat', 'bird', 'pottedplant', \n",
    "    'sheep', 'boat', 'aeroplane', 'tvmonitor', 'bicycle', 'sofa', \n",
    "    'horse', 'motorbike', 'diningtable', 'cow', 'train', 'bus',\n",
    "]\n",
    "\n",
    "cls2idx = {k: i for i, k in enumerate(CLASSES)}\n",
    "\n",
    "\n",
    "def process_image(pil_image):\n",
    "    img = np.asarray(pil_image)\n",
    "    img = img.astype(np.float32) / 255.0 # img \\in [0, 1]\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
    "    std =  np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n",
    "    img = (img - mean) / std\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    return img\n",
    "\n",
    "class Verydet:\n",
    "    def __init__(self, root, image_set=\"train\", download=True):\n",
    "        self.dataset = VOCDetection(root, image_set=image_set, download=download)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        pil_image, ddict = self.dataset[item]\n",
    "        img = process_image(pil_image)\n",
    "        img = torch.tensor(img)        \n",
    "        ddict = ddict['annotation']\n",
    "        \n",
    "        objects = []\n",
    "        bboxes = []\n",
    "        for x in ddict['object']:\n",
    "            objects.append(cls2idx[x['name']])\n",
    "            bb = {k: int(v) for k, v in x['bndbox'].items()}\n",
    "            bboxes.append(tuple(bb[k] for k in ['xmin', 'ymin', 'xmax', 'ymax']))\n",
    "        ret = {\"img\": img, \"cls\": objects, \"bboxes\": bboxes}\n",
    "        return ret\n",
    "\n",
    "\n",
    "trainset = Verydet(\"./voc\", image_set=\"train\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[ 2.2489,  2.2489,  2.2489,  ...,  1.3413,  1.3584,  1.3755],\n",
       "          [ 2.2489,  2.2489,  2.2489,  ...,  1.3584,  1.3584,  1.3413],\n",
       "          [ 2.2489,  2.2489,  2.2489,  ...,  1.4098,  1.3927,  1.3927],\n",
       "          ...,\n",
       "          [ 1.3927,  1.2043,  1.4098,  ...,  0.2282, -0.0629, -0.0801],\n",
       "          [ 1.1700,  1.1872,  1.1872,  ..., -0.2171, -0.2684, -0.0287],\n",
       "          [ 0.9303,  0.9646,  1.0502,  ..., -0.7137, -0.8678, -0.7479]],\n",
       " \n",
       "         [[ 2.4286,  2.4286,  2.4286,  ...,  1.5532,  1.5707,  1.5882],\n",
       "          [ 2.4286,  2.4286,  2.4286,  ...,  1.5707,  1.5707,  1.5532],\n",
       "          [ 2.4286,  2.4286,  2.4286,  ...,  1.6232,  1.6057,  1.6057],\n",
       "          ...,\n",
       "          [ 1.3081,  1.1155,  1.3256,  ...,  0.2052, -0.0749, -0.0574],\n",
       "          [ 1.0805,  1.0980,  1.0980,  ..., -0.1800, -0.2325,  0.0476],\n",
       "          [ 0.8354,  0.8704,  0.9580,  ..., -0.6877, -0.8452, -0.6877]],\n",
       " \n",
       "         [[ 2.6400,  2.6400,  2.6400,  ...,  2.5180,  2.5354,  2.5529],\n",
       "          [ 2.6400,  2.6400,  2.6400,  ...,  2.5354,  2.5354,  2.5180],\n",
       "          [ 2.6400,  2.6400,  2.6400,  ...,  2.5877,  2.5703,  2.5703],\n",
       "          ...,\n",
       "          [ 1.2980,  1.1062,  1.3154,  ...,  0.2696, -0.0441, -0.0441],\n",
       "          [ 1.0714,  1.0888,  1.0888,  ..., -0.0615, -0.1138,  0.1128],\n",
       "          [ 0.8274,  0.8622,  0.9494,  ..., -0.5321, -0.6890, -0.5844]]]),\n",
       " 'cls': [14, 0],\n",
       " 'bboxes': [(53, 87, 471, 420), (158, 44, 289, 167)]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WOK \n",
    "_Pascal VOC EDA_\n",
    "\n",
    "Просто посмотреть, что у нас в датасете лежит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a6017f7e7d450eab26f6dbf23402e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5717.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ds = VOCDetection(\"./voc\", image_set=\"train\")\n",
    "\n",
    "some_stats = defaultdict(list)\n",
    "N = len(ds)\n",
    "for i in trange(N):\n",
    "    pic, ddict = ds[i]\n",
    "    anno = ddict['annotation']\n",
    "    w = int(anno['size']['width'])\n",
    "    h = int(anno['size']['height'])\n",
    "    d = int(anno['size']['depth'])\n",
    "    \n",
    "    some_stats['w'].append(w)\n",
    "    some_stats['h'].append(h)\n",
    "    some_stats['d'].append(d)\n",
    "    \n",
    "    some_stats['objects_per_image'].append(len(anno['object']))\n",
    "    for x in anno['object']:\n",
    "        name = x['name']\n",
    "        bb = x['bndbox']\n",
    "        ww = int(bb['xmax']) - int(bb['xmin'])\n",
    "        hh = int(bb['ymax']) - int(bb['ymin'])\n",
    "        aspect = (ww / hh + 1e-5)\n",
    "        rel_ww = ww / w\n",
    "        rel_hh = hh / h\n",
    "        \n",
    "        some_stats['name'].append(name)\n",
    "        some_stats['ww'].append(ww)\n",
    "        some_stats['hh'].append(hh)\n",
    "        some_stats['aspect'].append(aspect)\n",
    "        some_stats['rel_ww'].append(rel_ww)\n",
    "        some_stats['rel_hh'].append(rel_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'chair', 'car', 'dog', 'bottle', 'cat', 'bird', 'pottedplant', 'sheep', 'boat', 'aeroplane', 'tvmonitor', 'bicycle', 'sofa', 'horse', 'motorbike', 'diningtable', 'cow', 'train', 'bus']\n"
     ]
    }
   ],
   "source": [
    "# names\n",
    "print(sorted(Counter(some_stats['name']).items(), key=lambda t: -t[1]))\n",
    "del some_stats['name']\n",
    "for k, v in some_stats.items():\n",
    "    pass\n",
    "#     plt.figure()\n",
    "#     plt.title(k)\n",
    "#     sns.distplot(v, kde=False)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 batch_size: int = 128):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        run_folder = Path(os.get_cwd())\n",
    "        train_log_folder = run_folder / \"train_log\"\n",
    "        val_log_folder = run_folder / \"val_log\"\n",
    "        print(f\"Run output folder is {run_folder}\")\n",
    "        os.makedirs(run_folder)\n",
    "        os.makedirs(train_log_folder)\n",
    "        os.makedirs(val_log_folder)\n",
    "\n",
    "        self.outpath = outpath\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.train_writer = SummaryWriter(log_dir=str(train_log_folder))\n",
    "        self.val_writer = SummaryWriter(log_dir=str(val_log_folder))\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = model.get_optimizer()\n",
    "\n",
    "        train_loader = model.get_loader(train=True)\n",
    "        val_loader = model.get_loader(train=False)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.post_train_batch()\n",
    "                for k, v in details.items():\n",
    "                    self.train_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "            model.post_val_stage(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                self.save_checkpoint(str(self.run_folder / \"best_checkpoint.pth\"))\n",
    "                best_loss = val_loss\n",
    "\n",
    "    def find_lr(self, min_lr: float = 1e-6,\n",
    "                max_lr: float = 1e-1,\n",
    "                num_lrs: int = 20,\n",
    "                smooth_beta: float = 0.8) -> dict:\n",
    "        lrs = np.geomspace(start=min_lr, stop=max_lr, num=num_lrs)\n",
    "        logs = {'lr': [], 'loss': [], 'avg_loss': []}\n",
    "        avg_loss = None\n",
    "        model = self.model\n",
    "        optimizer = model.get_optimizer()\n",
    "        train_loader = model.get_loader(train=True)\n",
    "\n",
    "        model.train()\n",
    "        for lr, batch in tqdm(zip(lrs, train_loader), desc='finding LR', total=num_lrs):\n",
    "            # apply new lr\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            # train step\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            loss, details = model.compute_all(batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate smoothed loss\n",
    "            if avg_loss is None:\n",
    "                avg_loss = loss\n",
    "            else:\n",
    "                avg_loss = smooth_beta * avg_loss + (1 - smooth_beta) * loss\n",
    "\n",
    "            # store values into logs\n",
    "            logs['lr'].append(lr)\n",
    "            logs['avg_loss'].append(avg_loss)\n",
    "            logs['loss'].append(loss)\n",
    "\n",
    "        logs.update({key: np.array(val) for key, val in logs.items()})\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запустите тренировку, подберите LR\n",
    "# Добавьте вывод bbox'ов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
