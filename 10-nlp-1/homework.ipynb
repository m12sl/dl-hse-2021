{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv2skd3-53rx"
   },
   "source": [
    "# Нейросети в задачих обработки текстов\n",
    "\n",
    "Основано на коде из курса [Глубинное обучение ФКН](https://github.com/aosokin/dl_cshse_ami).\n",
    "\n",
    "**Разработчик: Алексей Озерин, Ирина Сапарина**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozLuJF3kIaPF"
   },
   "source": [
    "# Генерация коротких текстов с помощью RNN и Transformer\n",
    "\n",
    "\n",
    "Генерировать тексты можно как с помощью RNN, так и с помощью Transformer, предсказывая следующий символ последовательности по предыдущим. Мы будем использовать архитектуру Transformer.\n",
    "\n",
    "В этом задании предлагается написать и проучить на небольшом датасете имен генеративную модель на основе символов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_s_Z5lbIaPG"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "torch.manual_seed(2021)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6nXxU8WIaPM"
   },
   "source": [
    "В файле `names` находится ~8k имен на латинице.\n",
    "\n",
    "Модель будет получать на вход имя `Amandy` и выдавать его же, только со сдвигом: `mandy `.\n",
    "\n",
    "Чтобы сеть училась генерировать заглавные буквы, добавим в начало специальный токен `_`.\n",
    "\n",
    "Также нам потребуется правило для останова генерации (это может быть просто ограничение на количество шагов). С другой стороны, можно добавить в конец каждого примера обучающей выборки специальный `<EOS>` токен. В данном случае обозначим его `#`:\n",
    "\n",
    "```\n",
    "_Amandy --> Amandy#\n",
    "```\n",
    "\n",
    "Можно прекращать генерацию при досрочном выпадании `<EOS>`.\n",
    "\n",
    "Для генерации на каждом шаге будем подавать на вход букву, предсказанную на предыдущем.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFRHva2zIaPN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "start_token = \"_\"\n",
    "eos = '#'\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.readlines()\n",
    "    names = [start_token + name.strip() + eos for name in names]\n",
    "\n",
    "names = list(set(names))  # в датасете есть повторы\n",
    "print('There are {} names: '.format(len(names)))\n",
    "for x in names[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RizB5cBTIaPP"
   },
   "source": [
    "# Подготовка и знакомство с данными\n",
    "**(0.1 балла)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSve0HBaIaPS"
   },
   "outputs": [],
   "source": [
    "# TODO: постройте частоты употреблений букв\n",
    "<your code>\n",
    "# HINT: для графика возьмите plt.bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAeSKss4IaPV"
   },
   "outputs": [],
   "source": [
    "# в датасете есть слова с разными длинами\n",
    "MAX_LENGTH = max(map(len,names))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len,names)), bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWnDPWr9IaPY"
   },
   "outputs": [],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgB0VE9BIaPa"
   },
   "outputs": [],
   "source": [
    "# TODO: отберите уникальные токены и заполните два словаря для конвертации токенов <-> индексы\n",
    "# сделайте так, чтобы pad_token имел номер 0\n",
    "    \n",
    "tokens = <your code>\n",
    "    \n",
    "tok2id = <your code>\n",
    "id2tok = <your code>\n",
    "\n",
    "n_tokens = len(tokens)\n",
    "print ('There are {} tokens',n_tokens)\n",
    "\n",
    "assert 50 < n_tokens < 60\n",
    "\n",
    "print('Vocabular: ' + \"\".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW62jy6xIaPm"
   },
   "source": [
    "## Работа с последовательностями произвольной длины в pytorch\n",
    "\n",
    "Нам нужно уметь генерировать батчи тензоров `[bs, 1, seq_len]`.\n",
    "Но в нашем датасете семплы разной длины:\n",
    "\n",
    "- мы могли бы подрезать все до минимальной\n",
    "- паддить до максимальной\n",
    "- выбрать какую-то среднюю длину\n",
    "\n",
    "**(0.1 балла)** Разбейте датасет на train и validate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем датасет выдающий закодированные имена:\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        entry = self.names[item]\n",
    "        return dict(\n",
    "            encoded=entry,\n",
    "        )\n",
    "\n",
    "encoded = []\n",
    "for entry in tqdm(names):\n",
    "    encoded.append(tok2id(entry))\n",
    "\n",
    "<your code>\n",
    "trainset = NamesDataset(...)\n",
    "valset = NamesDataset(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте соберем наивный DataLoader и посмотрим как он делает батчи:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "it = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(it)['encoded']\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В моем случае, результат запуска был таков:\n",
    "```\n",
    "[tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
    " tensor([ 6,  7,  6, 15,  5,  6,  5, 62]),\n",
    " tensor([ 48,  34,  83,   7,  32, 221,  22,  43]),\n",
    " tensor([  5, 143,  37,  36, 129,  12,  11,  66]),\n",
    " tensor([  73, 1258,  279,    8,    6,  555,   41,   10]),\n",
    " tensor([  8, 140,   8, 628,  20,  96,  13, 270]),\n",
    " tensor([  47,    4,   15,   18,   55,  269,    6, 1287]),\n",
    " tensor([ 58,   2,  13, 140, 193, 140, 171, 140])]\n",
    "```\n",
    "\n",
    "Какие странности здесь видны?\n",
    "1. Это не тензор, а список тензоров. Соответственно при итерировании по нулевой размерности (`batch[i, :]`) мы будем получать не i-пример, а i-токены для всех примеров в батче. Это не проблема, но отличается от ожидаемого поведения.\n",
    "2. На `<EOS>` (2) оканчивается только один пример, остальные подрезаны под его длину. И вот это уже проблема.\n",
    "\n",
    "Мы бы хотели западдить все примеры до длины максимального в батче. \n",
    "Но на этапе подготовки примера (в функции `__getitem__`) мы не знаем соседей по батчу!\n",
    "Для того чтобы поменять логику склейки батчей нам понадобиться написать свою функцию `collate_fn` в конструкторе DataLoader:\n",
    "\n",
    "```\n",
    "def collate_fn(samples):\n",
    "    # samples -- список семплов-словарей\n",
    "    <...>\n",
    "    return batch\n",
    "```\n",
    "\n",
    "**(0.1 балл)** Напишите функцию `collate_fn`, которая _правильно_ паддит names-последовательности и объединяет их в батчи, где `batch[i, :]` выдает токены для `i`-примера.\n",
    "\n",
    "Ожидаемый выход (для последовательности с левым паддингом):\n",
    "\n",
    "```\n",
    "tensor([[   1,   10, 3429,  405,  113,  676,   10, 1031,  140,    4,    2],\n",
    "        [   0,    1,   57,   18,   23,   19,   61,    7,  140,    4,    2],\n",
    "        [   0,    0,    0,    1,   16,   17, 1131,  416,  140,    4,    2],\n",
    "        [   0,    0,    0,    1,   13,  465,   75,  197,  140,    4,    2],\n",
    "        [   0,    0,    0,    1,    6,  302,   13,  144,  140,    4,    2],\n",
    "        [   0,    1,    6,   59,  205,  167,    8,   15,  140,    4,    2],\n",
    "        [   0,    0,    0,    0,    1,    6,   14,  678,  140,    4,    2],\n",
    "        [   0,    0,    1,    5,   29,   67,    6,   14,  140,    4,    2]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    # <your code>\n",
    "    return dict(\n",
    "        encoded=...\n",
    "    )\n",
    "    \n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "it = iter(trainloader)\n",
    "next(it)['encoded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN для имен (0.2 балла)\n",
    "\n",
    "Вам нужно написать сеть, кодирующую номера входных символов с помощью таблицы Embeddings. \n",
    "Получившиеся тензоры пропустить через RNN ячейку, затем преобразовать в логиты для предсказания номера нового символа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfGnm2QoIaPo"
   },
   "outputs": [],
   "source": [
    "# NB: обратите внимание на порядок осей при вызове forward\n",
    "# http://pytorch.org/docs/master/nn.html#recurrent-layers\n",
    "\n",
    "# Сделайте возможность выбора типа ячейки, RNN, GRU или LSTM\n",
    "# TODO: заполните пропуски. Функция forward будет вызываться на каждый шаг нами\n",
    "\n",
    "class NameRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, output_size, cell=\"lstm\", n_layers=1):\n",
    "        super(NameRNN, self).__init__()\n",
    "        # добавьте возможность выбрать тип ячейки RNN/LSTM\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        <...>\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        <...>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Натренируйте модель (0.2 балла)\n",
    "\n",
    "Возьмите трейнер с предыдущих занятий, натренируйте модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffTAktWAIaP5"
   },
   "source": [
    "# Генерация по argmax (0.2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pElLbEKIaQD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugRlkX2ZIaP6"
   },
   "outputs": [],
   "source": [
    "# Напишите функцию генерации продолжения строки\n",
    "def pick_by_argmax(logits):\n",
    "    <your code>\n",
    "\n",
    "def ids2string(ids):\n",
    "    return \"\".join(id2tok[_] for _ in ids)\n",
    "\n",
    "\n",
    "def gen_continuation(model, prefix=\"_\"):\n",
    "       # TODO: сначала подайте на вход префикс\n",
    "    # нас интересует последний output, чтобы получить первое предсказание\n",
    "    <your code>\n",
    "    \n",
    "    # TODO: затем сгенерируйте несколько последующих символов\n",
    "    # outs -- это массив с номерами токенов\n",
    "    <your code>\n",
    "    \n",
    "    print(prefix + '|'+ ids2string(outs))\n",
    "    \n",
    "gen_continuation(model, \" Ku\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00547AA-IaP_"
   },
   "source": [
    "# Генерация с семплированием (0.2 балла)\n",
    "\n",
    "Обычный софтмакс \n",
    "$$p_i = \\frac{\\exp (x_i)}{\\sum \\exp (x_j)}$$\n",
    "можно модернизировать с помощью температуры:\n",
    "$$p_i = \\frac{\\exp (x_i / T)}{\\sum \\exp (x_j / T)}$$\n",
    "\n",
    "Это позволит плавно переходить от выбора наиболее вероятного элемента ($T << 1$) до практически равновероятного ($T >> 1$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71cOcFxpIaQA"
   },
   "outputs": [],
   "source": [
    "# Напишите функцию генерации батчами с семплированием из распределения и температурой\n",
    "def batch2string(ids, prefix):\n",
    "    # модифицируйте ids2string для работы с батчами\n",
    "    <your code>\n",
    "\n",
    "def pick_by_distribution(logits):\n",
    "    # превратите логиты в распределение\n",
    "    # затем семлируйте из него batch примеров\n",
    "    <your code>\n",
    "\n",
    "\n",
    "def gen_continuation_temp(model, prefix=\"_\", temperature=1.0, n=10):\n",
    "    # аналогично, сначала подайте на вход префикс\n",
    "    # нас интересует последний output, чтобы получить первое предсказание\n",
    "    <your code>\n",
    "    \n",
    "    # затем, сгенерируйте n последующих символов\n",
    "    # в outs положите матрицу номеров токенов и отобразите ее\n",
    "    \n",
    "    print(batch2string(outs, prefix + '|'))\n",
    "    \n",
    "gen_continuation_temp(model, prefix=\" An\", temperature=0.5, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhgqoEHOIaPr"
   },
   "source": [
    "# Char-Transformer для имен (1.0 дополнительные баллы)\n",
    "\n",
    "Вам нужно написать сеть, кодирующую входные символы и их позиции с помощью таблиц Embeddings. \n",
    "Получившиеся тензоры пропустить через `TransformerEncoder`, затем преобразовать в логиты для предсказания новых символов.\n",
    "\n",
    "Transformer может обрабатывать сразу всю последовательность за один проход. Для того, чтобы у модели не было возможности \"заглянуть в будущее\", то есть использовать информацию о впреди идущих символах, необходимо сгенерировать маску. `TransformerEncoder` должен принимать на вход последовательность символов и маску.    \n",
    "![Transformer](https://drive.google.com/uc?export=view&id=1gXILzT3mGgc0mGlvqY-6R4bGs3Lx2YxM)\n",
    "Картинка из [illustrated transformer](http://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJCf0LYIIaPt"
   },
   "outputs": [],
   "source": [
    "# TODO: заполните пропуски\n",
    "\n",
    "class NameTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, n_layers=2, n_head=2, dropout=0.1):\n",
    "        super(NameTransformer, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        <your code>\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, seq_len):\n",
    "        # TODO: сгенерируйте маску размера seq_len x seq_len\n",
    "        # если во время кодирования i-го символа j-й символ доступен, \n",
    "        # то (i,j) элемент маски равен 0, иначе -inf\n",
    "        \n",
    "        <your code>\n",
    "\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        <your code>\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmkgMHc8IaPu"
   },
   "source": [
    "# Натренируйте модель\n",
    "\n",
    "И убедитесь, что она работает адекватно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sxrc0a10IaPy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL20-fall-seminar6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
