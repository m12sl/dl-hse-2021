{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика обучения моделей 2\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m12sl/dl-hse-2021/blob/master/03-training/seminar.ipynb)\n",
    "\n",
    "\n",
    "План семинара:\n",
    "\n",
    "- [ ] Освоить LR scheduling\n",
    "- [ ] Написать LR range test\n",
    "- [ ] Разобраться с подсчетом валидационных и тренировочных метрик \n",
    "- [ ] Classier Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Scheduling\n",
    "\n",
    "Два типа расписаний:\n",
    "\n",
    "- по эпохам (StepLR, ReduceLROnPlateau, ...) \n",
    "    ```\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        train(...)\n",
    "        validate(...)\n",
    "        scheduler.step()\n",
    "    ```\n",
    "\n",
    "\n",
    "- по батчам (Cosine, Cyclic, 1cycle, ...)\n",
    "    ```\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n",
    "    for epoch in range(epochs):\n",
    "        # train(...)\n",
    "        for batch in data_loader:\n",
    "            train_batch(...)\n",
    "            scheduler.step()\n",
    "        # validate(...)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор оптимального LR\n",
    "\n",
    "\n",
    "Для выбора оптимального LR удобно использовать т.н. Learning Rate Range Test, часто процедуру называют просто find_lr. Под капотом проход по тренировочной эпохе с lr, изменяемым на каждом батче по формуле:\n",
    "\n",
    "$$\n",
    "\\mathrm{it} = \\frac{\\mathrm{step}}{\\mathrm{total steps}}\\\\\n",
    "\\mathrm{lr} = \\exp\\left\\{ \n",
    "    (1 - t ) \\log a + t \\log b\n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "Чтобы поменять LR для всех оптимизируемых параметров, можно пройтись по ним циклом:\n",
    "\n",
    "```\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "```\n",
    "\n",
    "\n",
    "<img src=\"https://www.jeremyjordan.me/content/images/2018/02/lr_finder.png\"/>\n",
    "\n",
    "_картинка из бложика [Jeremy Jordan](https://www.jeremyjordan.me/nn-learning-rate/)_\n",
    "\n",
    "\n",
    "Идея приема простая: пока LR меньше некоторого порога на каждом шаге градиентного спуска веса просто не меняются (в частности из-за особенностей операций с плавающей точкой).\n",
    "При очень большом LR мы шагаем слишком далеко и уходим от точки экстремума. \n",
    "\n",
    "Оптимальный LR лежит где-то между ними. Экспоненциальная формула изменения LR позволяет с должным качеством найти хорошую точку.\n",
    "\n",
    "\n",
    "\n",
    "Если интересно: [статья , в которой эту технику предложили и активно использовали](https://arxiv.org/pdf/1506.01186.pdf).\n",
    "\n",
    "\n",
    "**Some math notes**\n",
    "\n",
    "У типов данных с плавающей точкой есть арифметические особенности:\n",
    "\n",
    "$$\n",
    "# fp32\n",
    "x + \\delta == x,\\,\\mathrm{если}\\; \\delta < 5.96 \\cdot 10^{-8} x\n",
    "$$\n",
    "\n",
    "К слову, это еще одна причина присматривать за величинами активаций, нормировать данные и таргет в случае регрессии. Можно было бы перейти на float64, но (вычислительно и по памяти) дешевле быть аккуратными на float32.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://blogs.nvidia.com/wp-content/uploads/2020/05/tf32-Mantissa-chart-hi-res-FINAL-400x255.png.webp\"/>\n",
    "\n",
    "_картинка из статьи [NVIDIA](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики\n",
    "\n",
    "TL; DR:\n",
    "- тренировочные метрики записывать без сглаживания с каждого батча\n",
    "- валидационные собирать за всю валидацию и рисовать одной точкой\n",
    "\n",
    "\n",
    "**Особенности TB**:\n",
    "\n",
    "- При отображении прореживает точки по global_step\n",
    "- Чтобы рисовать на одном графике надо писать в разные папки (завести отдельные train_ и val_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обновим Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryModel(nn.Module):\n",
    "    def __init__(self, lr_scheduler=None, lr_scheduler_type=None):\n",
    "        super().__init__()\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.lr_scheduler_type = lr_scheduler_type\n",
    "        if lr_scheduler_type not in [None, 'per_batch', 'per_epoch']:\n",
    "            raise ValueError(\"lr_scheduler_type must be one of: None, 'per_batch', 'per_epoch'. \"\n",
    "                             f\"Not: {lr_scheduler_type}\")\n",
    "\n",
    "        self.inner = nn.Sequential(nn.Linear(784, 100),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(100, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.inner(x)\n",
    "\n",
    "    def compute_all(self, batch):  # удобно сделать функцию, в которой вычисляется лосс по пришедшему батчу\n",
    "        x = batch['sample'] / 255.0\n",
    "        y = batch['label']\n",
    "        logits = self.inner(x)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(axis=1) == y).float().mean().cpu().numpy()\n",
    "        metrics = dict(acc=acc)\n",
    "\n",
    "        return loss, metrics\n",
    "\n",
    "    def post_train_batch(self):\n",
    "        # called after every train batch\n",
    "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_batch':\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "    def post_val_batch(self):\n",
    "        pass\n",
    "\n",
    "    def post_train_stage(self):\n",
    "        pass\n",
    "\n",
    "    def post_val_stage(self, val_loss):\n",
    "        # called after every end of val stage (equals to epoch end)\n",
    "        if self.lr_scheduler is not None and self.lr_scheduler_type == 'per_epoch':\n",
    "            self.lr_scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 optimizer,\n",
    "                 train_dataset: Dataset,\n",
    "                 val_dataset: Dataset,\n",
    "                 tboard_log_dir: str = './tboard_logs/',\n",
    "                 batch_size: int = 128):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.log_writer = SummaryWriter(log_dir=tboard_log_dir)\n",
    "        self.cache = self.cache_states()\n",
    "\n",
    "    def save_checkpoint(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def train(self, num_epochs: int):\n",
    "        model = self.model\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, pin_memory=True, batch_size=self.batch_size)\n",
    "        val_loader = DataLoader(self.val_dataset, shuffle=False, pin_memory=True, batch_size=self.batch_size)\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                model.post_train_batch()\n",
    "                for k, v in details.items():\n",
    "                    self.log_writer.add_scalar(k, v, global_step=self.global_step)\n",
    "                self.global_step += 1\n",
    "\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for batch in tqdm(val_loader):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                loss, details = model.compute_all(batch)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "            val_loss = np.mean(val_losses)\n",
    "            model.post_val_stage(val_loss)\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                self.save_checkpoint(\"./best_checkpoint.pth\")\n",
    "                best_loss = val_loss\n",
    "\n",
    "    def find_lr(self, min_lr: float = 1e-6,\n",
    "                max_lr: float = 1e-1,\n",
    "                num_lrs: int = 20,\n",
    "                smooth_beta: float = 0.8):\n",
    "        lrs = np.geomspace(start=min_lr, stop=max_lr, num=num_lrs)\n",
    "        logs = {'lr': [], 'loss': [], 'avg_loss': []}\n",
    "        avg_loss = None\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        train_loader = DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "        iter_dataloader = iter(train_loader)\n",
    "\n",
    "        model.train()\n",
    "        for iter_step in tqdm(range(num_lrs), desc='finding LR', total=num_lrs):\n",
    "            # apply new lr\n",
    "            cur_lr = lrs[iter_step]\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = cur_lr\n",
    "\n",
    "            # train step\n",
    "            batch = next(iter_dataloader)\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            loss, details = model.compute_all(batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate smoothed loss\n",
    "            if iter_step == 0:\n",
    "                avg_loss = loss\n",
    "            else:\n",
    "                avg_loss = smooth_beta * avg_loss + (1 - smooth_beta) * loss\n",
    "\n",
    "            # store values into logs\n",
    "            logs['lr'].append(cur_lr)\n",
    "            logs['avg_loss'].append(avg_loss)\n",
    "            logs['loss'].append(loss)\n",
    "\n",
    "        logs.update({key: np.array(val) for key, val in logs.items()})\n",
    "        self.rollback_states()\n",
    "\n",
    "    def cache_states(self):\n",
    "        cache_dict = {'model_state': deepcopy(self.model.state_dict()),\n",
    "                      'optimizer_state': deepcopy(self.optimizer.state_dict())}\n",
    "\n",
    "        return cache_dict\n",
    "\n",
    "    def rollback_states(self):\n",
    "        self.model.load_state_dict(self.cache['model_state'])\n",
    "        self.optimizer.load_state_dict(self.cache['optimizer_state'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}